{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo dataset OFFCOMBR-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16L13Z0StmPtcTv8pu3AdDjP4rTlf16PE",
      "authorship_tag": "ABX9TyP2j0pUvB3DKDmO+yDi26gp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Felix-Leonel/Codigos-Hate-Speech/blob/main/Codigo_dataset_OFFCOMBR_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRI-Ak1anWcL",
        "outputId": "9f246ba0-5cbc-41bb-e0a3-8d9354b56fa8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t30B7daxnvcu",
        "outputId": "5c3e001c-a767-4c15-b4e1-32106b4c639e"
      },
      "source": [
        "!pip install transformers\n",
        "!pip uninstall tokenizers\n",
        "!pip install tokenizers\n",
        "\n",
        "import torch\n",
        "from transformers import BertModel, BertForMaskedLM, PreTrainedTokenizer\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import logging\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# USA O MODULO TOKENIZER DO HUGGINFACE\n",
        "from tokenizers import (ByteLevelBPETokenizer,\n",
        "                            SentencePieceBPETokenizer,\n",
        "                            BertWordPieceTokenizer)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#IMPORTA O BERT E O OTIMIZADOR ADAM\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel\n",
        "\n",
        "#ESTA CLASSE FARÁ O AGENDAMENTO\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import accuracy_score  \n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 46.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
            "Uninstalling tokenizers-0.10.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/tokenizers-0.10.3.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tokenizers/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tokenizers-0.10.3\n",
            "Collecting tokenizers\n",
            "  Using cached https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
            "Installing collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9675 sha256=09e09626fb532f6e8355fd627f6469d6afb87f5f7108499c1c949500a60dfffa\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1otfY7nKn0Oi",
        "outputId": "27dca410-439b-4f2e-fc15-d2e24b6beaf9"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"#Configura o ambiente para usar GPU (CUDA)\"\"\"\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\"\"\"OPCIONAL: Pode ativar um debbug para entender melhor o que acontece\"\"\"\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\"\"\"Carrega um modelo de Tokenizer pré-treinado (neste caso o Multilanguage para trabalhar com português)\n",
        "#Download do Dataset de treinamento\n",
        "Fazer o download do CoLA Dataset ( The Corpus of Linguistic Acceptability (CoLA)). Um dataset em inglês para classificação simples: se a senteça está gramaticalmente correta ou não (parte do GLUE Benchmark)\n",
        "Instala o Wget para fazer o download\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"#Pre-processando o Dataset\n",
        "Na pasta, existem duas pastas. Precisamos usar os dados brutos (raw) para que possamos criar os token usando o BertTokenizer. Não podemos usar as setenças tokenizadas porque o processo utilizado na criação desses tokens foi diferente do usado pelo Bert.<br>\n",
        "Iremos utilizar o pandas para manipular as entradas.\n",
        "\"\"\"\n",
        "############################################################################################\n",
        "df = pd.read_csv('/content/drive/MyDrive/Python/OFFCOMBR_CSV/OffComBR3.csv', delimiter=',')\n",
        "df['@@class'].replace('yes', 1, inplace=True)\n",
        "df['@@class'].replace('no', 0, inplace=True)\n",
        "#def augment_text(df,samples=629,pr=0.2):\n",
        "#    aug_w2v.aug_p=pr\n",
        "#    new_text=[]\n",
        "#    \n",
        "#    ##selecting the minority class samples\n",
        "#    df_n=df[df.target==1].reset_index(drop=True)\n",
        "#\n",
        "#    ## data augmentation loop\n",
        "#    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n",
        "#        \n",
        "#            text = df_n.iloc[i]['document']\n",
        "#            augmented_text = aug_w2v.augment(text)\n",
        "#            new_text.append(augmented_text)\n",
        "#    \n",
        "#    \n",
        "#    ## dataframe\n",
        "#    new=pd.DataFrame({'text':new_text,'target':1})\n",
        "#    df=shuffle(df.append(new).reset_index(drop=True))\n",
        "#    return df\n",
        "#Retirando 'RT', '@' e # do arquivo\n",
        "#df['document'] = df['document'].replace('#\\w*', '', regex=True)\n",
        "#df['document'] = df['document'].replace('RT @\\w*: ', '', regex=True)\n",
        "#df['document'] = df['document'].replace('@\\w* ', '', regex=True)\n",
        "########################################################################\n",
        "\"\"\"Podemos criar dos arrays (Numpy Array) para armazenar as setenças e os labels. Também vamos deixar todos os textos em letras minúsculas:\"\"\"\n",
        "sentencas = df['document'].values\n",
        "labels = df['@@class'].values\n",
        "\n",
        "# MOSTRANDO O TIPO DE DADOS (Numpy Array)\n",
        "print(type(sentencas))\n",
        "\n",
        "\n",
        "#MODIFICANDO AS SETENÇAS PARA TODAS AS LETRAS MINÚSCULAS\n",
        "print(len(sentencas))\n",
        "for x in range (len(sentencas)):\n",
        "  sentencas[x] = sentencas[x].lower()\n",
        "print(sentencas[7])\n",
        "\n",
        "\"\"\"#Formatando a Entrada e Criando os Tokens e IDS\n",
        "Agora chegou a hora de criarmos os tokens para que depois eles possam ser mapeados para os index do vocabulário.<Br>\n",
        "Para isso, utilizaremos o BertTokenizer, que já importamos anteriormente. Utilizaremos a versão 'bert-based-unscased' (inglês).<br>\n",
        "Antes disso, no entanto, precisamos formatar a entrada para respeitar os requisitos de entrada do Bert, que são os seguintes:\n",
        "1.   Adicionar tokens especiais no início e no fim de cada sentença ([CLS] para o início de uma sentença de classificação / [SEP] para indicar o fim de uma sentença.\n",
        "![alt text](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n",
        "2. Precisamos fazer o Pad e Truncate para que todas as senteças tenham o mesmo tamanho de entrada (máximo de 512 tokens).\n",
        "3. Diferenciar o que são tokens reais dos token de padding, criando um \"attention mask\". \n",
        "![alt text](http://www.mccormickml.com/assets/BERT/padding_and_mask.png)\n",
        "#Entendendo o Tokenizer<br>\n",
        "O modelo do Google tem um vocabulário de 30.000 tokens com aprox 768 dimensões cada (embeedings).<br>\n",
        "Os Tokens podem representar palavras completas (mas não todas) <br>\n",
        "Os Tokens podem representar sub-palavras de palavras desconhecidas (as subwords são iniciadas com ##. Reparem que a palavra Diogo não existe no vocabulário. Ela é quebrada em duas subwords: Dio + ##go. Atenção: O token go é diferente do ##go) <oov><br>\n",
        "Os Tokens podem representar caracteres e marcações especiais:<br>\n",
        "[PAD]: Padding<br>\n",
        "[UNK]:Unknow<br>\n",
        "[CLS]: Classificação<br>\n",
        "[SEP]: Separação das sentenças<br>\n",
        "[MASK]: Máscara para a palavra<br>\n",
        "\"\"\"\n",
        "\n",
        "# FAZ O DOWNLOAD DO VOCABULÁRIO\n",
        "if not os.path.exists('vocab.txt'):\n",
        "  wget.download(\"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt\")\n",
        "\n",
        "# FAZ O DOWNLOAD DO PRE-TREINADO EM PT-BT\n",
        "if not os.path.exists('bert-base-portuguese-cased_pytorch_checkpoint.zip'):\n",
        "  wget.download(\"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\")\n",
        "  !unzip bert-base-portuguese-cased_pytorch_checkpoint.zip -d bert-portuguese\n",
        "\n",
        "# CRIA O TOKENIZER A PARTIR DE UM VOCABULÁRIO\n",
        "# LOWERCASE = FALSE (NÃO IRÁ CONVERTER AS ENTRADAS PARA LOWERCASE. MANTEM O ORGINIAL)\n",
        "# STRIP ACCENTS = FALSE (MANTEM OS ACENTOS)\n",
        "tokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=False, strip_accents=False)\n",
        "\n",
        "# MOSTRA AS INFORMAÇÕES DO TONENIZER\n",
        "print(tokenizer)\n",
        "\n",
        "# PERMITE O TRUNCATION E O PADDING\n",
        "tokenizer.enable_truncation(max_length=240)\n",
        "tokenizer.enable_padding()\n",
        "\n",
        "\n",
        "# TOKENINZA EM BATCH TODAS AS SENTENÇAS\n",
        "# TEM QUE USAR .TOLIST PARA CONVERTER POR LISTA. SENTENCAS É UM ARRAY NUMPY\n",
        "output = tokenizer.encode_batch(sentencas.tolist())\n",
        "\n",
        "# O TOKENIZER RETORAR UMA LISTA DE OBJETOS DO TIPO TOKENIZER\n",
        "# PRECISAMOS PEGAR OS ATRIBUTOS IDS E MASKS E ADICIONAR PARA LISTAS\n",
        "# OS OBJETOS TEM O ATRIBUTO IDS(IDS), TOKENS (TOKENS) E attention_mask\n",
        "# PRECISAMOS FAXER O FOR PARA PEGAR CADA UM E DEPOIS CRIAR A LISTA\n",
        "ids=[x.ids for x in output]\n",
        "attention_mask = [x.attention_mask for x in output]\n",
        "\n",
        "print(len(ids))\n",
        "print(len(attention_mask))\n",
        "\n",
        "# PRINTS EXEMPLO DE SAIDA DA PRIMEIRA LINHA\n",
        "print(output[0])\n",
        "print(output[0].tokens)\n",
        "\n",
        "\"\"\"#Dividindo o Dataset em Treinamento e Validação\n",
        "Vamos usar a ferramenta do ScikitLearn para nos ajudar neste processo. Vamos dividir o dataset em 80% para treinamento e 20% para a validação\n",
        "\"\"\"\n",
        "\n",
        "#USAR O MESMO RANDON_STATE PARA NAO TROCAR OS INPUTS DE SUAS MÁSCARAS\n",
        "train_input, validation_input, train_labels, validation_labels = train_test_split(ids, labels, random_state=2018, test_size=0.2)\n",
        "train_mask, validation_mask, _, _ = train_test_split(attention_mask, labels, random_state=2018, test_size=0.2)\n",
        "\n",
        "#COMPARANDO A PRIMEIRA LINHA DE TREINAMENTO COM A MASCARA\n",
        "print(train_input[0])\n",
        "print(train_mask[0])\n",
        "\n",
        "\"\"\"#Criando os tensores (Pytorch Data Type)\n",
        "Os modelos do PyTorch esperam de entrada o tipo tensor, então precisamos converter o nosso dataset de Numpy Array para tensores.\n",
        "\"\"\"\n",
        "\n",
        "train_input_tensor = torch.tensor(train_input)\n",
        "validation_input_tensor = torch.tensor(validation_input)\n",
        "\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "validation_labels_tensor = torch.tensor(validation_labels)\n",
        "\n",
        "train_mask_tensor = torch.tensor(train_mask)\n",
        "validation_mask_tensor= torch.tensor(validation_mask)\n",
        "\n",
        "\"\"\"Uma ação adicional é usar o torch DataLoade, que cria um \"iterator\". Diferente de um for, o iterador não sobe todo o dataset não precisa ser carregado todo na memória (ajuda no treinamento)\"\"\"\n",
        "\n",
        "#É PRECISO ESPECIFICAR O TAMANHO DO BATCH, PARA O BERT OS AUTORES RECOMENDAM 16 OU 32\n",
        "batch_size = 32\n",
        "\n",
        "#CRIA OS DATALOADERS PARA O CONJUNTO DE TREINAMENTO\n",
        "train_data = TensorDataset(train_input_tensor, train_mask_tensor, train_labels_tensor)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#CRIA OS DATALOADRES PARA O CONJUNTO DE VALIDAÇÃO\n",
        "validation_data = TensorDataset(validation_input_tensor, validation_mask_tensor, validation_labels_tensor)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "\"\"\"#Treinando o modelo\n",
        "O Bert oferece um modelo pré-treinando a qual só precisamos fazer fine-tune para a tarefas que desejamos. O huggingface disponibiza não só o modelo pré-treinado mas também interfaces para nossas tarefas específicas. Algumas disponíveis são:\n",
        "*   BertMode\n",
        "*   BertForMaskedLM\n",
        "*   BertForNextSentencePrediction\n",
        "*   BertForSequenceClassification (vamos usar este)\n",
        "*   BertForTokenClassification\n",
        "*   BertForQuestionAnsering\n",
        "O BerForSequenceClassification basicamente é a implementação do modelo Bert com a adição de uma camada de FFN para classificação. Lembre-se que o huggieface disponibilizou diversas versões de modelo pré-treinanda (base, large, multilanugage). Você pode escolher a que for melhor para o seu propósito. Neste caso, vamos utilizar a versão multilingual por contemplar o português.\n",
        "\"\"\"\n",
        "\n",
        "#CRIA O MODELO BERT PRETREINADO COM UMA CAMADA DE CLASSIFICAÇÃO NO TOPO\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-portuguese', \n",
        "    num_labels = 2, # NUMERO DE CLASSES (NO CASO BINÁRIA: ACEITÁVEL OU NÃO. PODE TER MAIS PARA MULTICLASSE)  \n",
        "    output_attentions = True, # SE O MODELO DEVE EXPORTAR OS PESOS DAS ATENÇÕES\n",
        "    output_hidden_states = True, # SE O MODELO DEVE EXPORTAR OS HIDDEN STATES (PODE SER INTERESSANTE PARA ESTUDAR EMBEDDINGS)\n",
        ")\n",
        "\n",
        "#DIZ AO MODELO PARA USAR GPU\n",
        "model.cuda()\n",
        "\n",
        "\"\"\"**CURIOSDIADE.** \n",
        "Uma curiosidade disponibilizada em (https://mccormickml.com/2019/07/22/BERT-fine-tuning/) <br>\n",
        "É possível mostrar os parâmetro do modelo:\n",
        "<br>\n",
        "*   A camada de embeddings\n",
        "*   A primeira das 12 camadas de transformers\n",
        "*   A camada de saída (output)\n",
        "A execução do trecho abaixo é optativa.\n",
        "\"\"\"\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "\"\"\"**Otimizador**\n",
        "Carregamos o modelo, agora precisamos criar o Otimizador Adam. Os autores recomendam os seguintres valores:\n",
        "*   Batch Size: 16, 32 (Lembre-se que usamos 32 no Dataloader)\n",
        "*   Learning Rate (Adam):  5e-5, 3e-5, 2e-5 (vamos usar 2e-5)\n",
        "*   Numero de épocas (Quantas vezes TODO o dataset é treinado):  2,3,4 (utilizaremos 4):\n",
        "\"\"\"\n",
        "\n",
        "#ADAMW É A CLASSE DO HUGGINGFACE\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "\"\"\"**Learning Rate Scheduler**\n",
        "Em redes neurais é útil diminuir a taxa de aprendizado (learning rate) conforme as épocas vão aumentando para que possamos evitar que o modelo entre em um estado \"caótico\" com taxas grandes ou o \"falso míninmo\" com taxas pequenas. A ideia é ir ajustando conforme as épocas vão passando.\n",
        "\"\"\"\n",
        "\n",
        "epochs = 4 #QUANTIDADE DE ÉPOCAS\n",
        "\n",
        "#PARA CALCULAR A QUANTIDADE DE PASSOS É A QTD DE BATCHS * ÉPOCAS\n",
        "total_steps = epochs * len(train_dataloader) \n",
        "\n",
        "#CRIANDO O AGENDADOR\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, #VALOR PADRÃO\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "\"\"\"**Loop de Treinamento**\n",
        "Não é só chamar alguma função para treinar o modelo. Precisamos criar um loop que se repita a quantidade de épocas especificadas executando as atividades abaixo. A cada passagem, também faremos uma avaliação do modelo:\n",
        "Loop de treinamento\n",
        "*   Desempacotar os dados de entrada e os labels\n",
        "*   Carregar os dados para a GPU\n",
        "*   Limpar os gradientes calculados na passagem anterior (no pytorch os gradientes são acumulados por padrão. pode ser útil para RNN, mas não no caso de transformers.\n",
        "* Forward Pass (Passar os dados pela rede)\n",
        "* Backward Pass (backpropagation)\n",
        "* Pedir para a rede atualizar os parâmetros (optimizer.step())\n",
        "* Monitar as variáveis para saber o progresso\n",
        "Loop de avaliação\n",
        "* Desempacotar os dados de entrada e os labels\n",
        "* Carregar os dados para a GPU\n",
        "* Forward Pass (Passar os dados pela rede)\n",
        "* Computar a perda na nossa validação e monitorar as variáveis para saber o progresso.\n",
        "**Antes, vamos criar duas funções de ajuda. Uma para calcular a acurácia do modelo e outra para formatar o horário**:\n",
        "\"\"\"\n",
        "# FUNÇÃO  QUE CALCULA A ACURÁCIA DO MODELO (PREDIÇÕES vs LABELS)\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# FUNÇÃO QUE FORMATA O HORÁRIO\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\"\"\"**AGORA VEM O LOOP DE TREINAMENTO :)**\"\"\"\n",
        "\n",
        "# PRIMEIRO PRECISAMOS GARANTIR A REPRODUTIBILIDADE\n",
        "# USANDO OS SEEDS DO PYTORCH, GARANTIMOS QUE OS VALORES SERÃO INICIADOS DA MESMA FORMA\n",
        "# VAMOS SETAR O MESMO VALOR EM DIFERENTES LUGARES\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# CRIANDO UMA LISTA QUE IRÁ ARMZENAR LOSS AO FIM DE CADA ÉPOCA\n",
        "loss_values = []\n",
        "\n",
        "# CRIANDO O LOOP DAS ÉPOCAS\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # MEDIR QUANTO TEMPO UMA ÉPOCA LEVA\n",
        "    t0 = time.time()\n",
        "\n",
        "    # RESETANDO O LOSS PARA ESTA ÉPOCA\n",
        "    total_loss = 0\n",
        "\n",
        "    #COLOCANDO O MODELO NO MODO DE TREINAMENTO\n",
        "    #ESSE COMANDO NÃO CHAMA O TREINAMENTO, APENAS AVISA O MODELO PARA FAZER AJUSTES DE DROPOUTS\n",
        "    model.train()\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # UM LOOP PARA CADA BATCH  DENTRO DA ÉPOCA\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # PRECISAMOS DESPEMPACOTAR O BATCH E CARREGAR NA GPU\n",
        "        # BATCH CONTEM TRÊS TENSORES\n",
        "        #   [0]: ID DE INPUT\n",
        "        #   [1]: ATTENTION MASKS\n",
        "        #   [2]: LABELS \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #PRECISAMOS LIMPAR O GRADIENTE ANTES DE BACKPROP\n",
        "        #PYTORCH NAO FAZ ISSO AUTOMÁTICO\n",
        "        model.zero_grad()\n",
        "\n",
        "        #AGORA VAMOS FAZER UMA PASSAGEM (FORWARD PASS)\n",
        "        #O RESULTADO SERÁ LOSS (NÃO SERÁ A PREDIÇÃO PQ PASSAMOS OS LABELS)\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, #USADO QUANDO É NEXT SEQUENCE\n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        #O MODELO RETORNA UMA TUPLA. \n",
        "        #VAMOS PEGAR O VALOR DA TUPLA\n",
        "        loss = outputs[0]\n",
        "        hidden_state = outputs[2]\n",
        "        \n",
        "\n",
        "        # VAMOS ARMAZENAR O HIDDEN STATE E ATENÇÃO TB\n",
        "      \n",
        "\n",
        "\n",
        "        #VAMOS ACUMULAR O VALOR NO TOTAL DE LOSS DA ÉPOCA\n",
        "        # .item() RETORNA UM VALOR PYTHON DE UM TENSOR\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        #AGORA VAMOS FAZER O BACKWARD PARA CALCULAR O GRADIENTE\n",
        "        loss.backward()\n",
        "\n",
        "        # PASSO NECESSÁRIO\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # O OTIMIZADOR VAI ATUALIZAR OS PARAMETROS COM BASE NO GRADIENTE\n",
        "        optimizer.step()\n",
        "\n",
        "        # ATUALIZANDO O LEARNING RATE\n",
        "        scheduler.step()\n",
        "\n",
        "    # APOS TODOS OS BATCH DE UMA EPOCA\n",
        "    # CACLULA AVERAGE LOSS COM BASE NO TREINAMENTO (TAMANHO DO DATASET)\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    #ARMAZENA O LOSS NA LISTA (PARA DEPOIS SER PLOTADO NO GRAFICO)\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    # DENTRO DE CADA ÉPOCA TAMBÉM VAMOS RODAR UMA AVALIAÇÃO\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # COLOCANDO O MODELO NO MODO DE AVALIAÇÃO (SAINDO DO MODULO DE TREINAMENTO)\n",
        "    model.eval()\n",
        "\n",
        "    # CRIANDO VARIÁVEIS DE MONITORAMENTO\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # LOOP PARA AVALIAR CADA BATCH DE TREINAMENTO\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # PEDE AO MODELO PARA NAO COMPUTAR GRADIENTES (É VALIDAÇÃO, NÃO TREINAMENTO)\n",
        "        with torch.no_grad():  \n",
        "          # FORWARD PASS PARA CALCULAR OS LOGITS DA PREDIÇÃO\n",
        "          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          \n",
        "        # O RESULTADO DO MODELO AGORA NÃO SERÁ LOSS\n",
        "        # SERÁ 'LOGITS', VALOR DE SAIDA ANTES DE UMA FUNÇÃO DE ATIVAÇÃO (SOFTMAX POR EXEMPLO)\n",
        "        # COMO É UMA CLASSIFICAÇÃO BINÁRIA, SÓ OS LOGITS SERVEM\n",
        "        # DEPENDENDO DO MODELO SERIA NECESSÁRIO UM SOFTMAX\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # MOVER OS LOGITS E OS LABELS PARA A CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        #CALCULAR ACURÁCIA CHAMANDO A FUNÇÃO QUE CRIAMOS ANTERIORMENTE\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # ACUMULAR O TOTAL DA ACURÁCIA\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # TRACKEAR O NUMERO DE BATCHS\n",
        "        nb_eval_steps +=1\n",
        "\n",
        "    # EXIBINDO DADOS FINAIS\n",
        "    print(\"  Acurácia: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Tempo de Validação: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "#FIM DAS EPOCAS\n",
        "print(\"FIM DO TREINAMENTO\")\n",
        "\n",
        "\"\"\"** Mostrando o gráfico LOSS por época **\"\"\"\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\"\"\"#Executando e Testando o modelo\n",
        "Agora que o modelo está treinado, podemos executá-lo. Nesta seção vamos fazer alguns testes. O primeiro ponto é preparar os dados (assim como fizemos na fase de treinamento), mas desta vez não precisamos dos labels.\n",
        "\"\"\"\n",
        "\n",
        "# FUNÇÃO QUE VALIDA O MODELO\n",
        "def Validar_Modelo(prediction_dataloader, batch_size):\n",
        "  #ARMAZENAR RESULTADOS\n",
        "  resultado_predicoes = []\n",
        "  resultados_esperados = []\n",
        "  falsos_positivos = []\n",
        "  falsos_negativos = []\n",
        "  verdadeiros_positivos = []\n",
        "  verdadeiros_negativos =[]\n",
        "  nb_eval_steps = 0\n",
        "  eval_accuracy = 0\n",
        "  tmp_eval_accuracy = 0\n",
        "  for batch in prediction_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    # PEDE AO MODELO PARA NAO COMPUTAR GRADIENTES (É VALIDAÇÃO, NÃO TREINAMENTO)\n",
        "    with torch.no_grad():  \n",
        "      # FORWARD PASS PARA CALCULAR OS LOGITS DA PREDIÇÃO\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)       \n",
        "    # O RESULTADO DO MODELO AGORA NÃO SERÁ LOSS\n",
        "    # SERÁ 'LOGITS', VALOR DE SAIDA ANTES DE UMA FUNÇÃO DE ATIVAÇÃO (SOFTMAX POR EXEMPLO)\n",
        "    # COMO É UMA CLASSIFICAÇÃO BINÁRIA, SÓ OS LOGITS SERVEM\n",
        "    # DEPENDENDO DO MODELO SERIA NECESSÁRIO UM SOFTMAX\n",
        "    # TAMBÉM É POSSIVEL PEGAR A ATENÇÃO E OS HIDDEN STATES\n",
        "    logits = outputs[0]\n",
        "    attention=outputs[-1]\n",
        "    all_hidden_state = outputs[-2]\n",
        "    \n",
        "    # MOVER OS LOGITS E OS LABELS PARA A CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    reconstruct_input_id = b_input_ids.to('cpu').numpy()\n",
        "\n",
        "    # LOOP PARA VISUALIZAR CADA SETENÇA\n",
        "    for logit, label, inputs, att in zip(logits, label_ids, reconstruct_input_id, attention):\n",
        "      resultado_predicoes.append(np.argmax(logit))\n",
        "      resultados_esperados.append(label)\n",
        "      # LOOP PARA IDENTIFICAR FALSOS POSITIVIOS, FALSOS NEGATIVOS e CORRETOS\n",
        "      if (label != np.argmax(logit)):\n",
        "        if (label == 1 and np.argmax(logit) == 0):\n",
        "          falsos_negativos.append(inputs)\n",
        "        elif(label == 0 and np.argmax(logit) == 1):\n",
        "          falsos_positivos.append(inputs)\n",
        "      else:\n",
        "        if (label == 1 and np.argmax(logit) == 1):\n",
        "          verdadeiros_positivos.append(inputs)\n",
        "        elif (label == 0 and np.argmax(logit) == 0):\n",
        "          verdadeiros_negativos.append(inputs)  \n",
        "\n",
        "    #CALCULAR ACURÁCIA CHAMANDO A FUNÇÃO QUE CRIAMOS ANTERIORMENTE\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "    # ACUMULAR O TOTAL DA ACURÁCIA\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    # TRACKEAR O NUMERO DE BATCHS\n",
        "    nb_eval_steps +=1    \n",
        "    #wait = input(\"PRESS ENTER TO CONTINUE.\")\n",
        "    \n",
        "\n",
        "  # RELATÓRIO FINAL\n",
        "  print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "  print(\"Accuracia: \", accuracy_score(resultados_esperados, resultado_predicoes))\n",
        "  \n",
        "  print(\"Balanced Accuracy:\", balanced_accuracy_score(resultados_esperados, resultado_predicoes))\n",
        "\n",
        "  print(\"F1 Score:\", f1_score(resultados_esperados, resultado_predicoes, average='weighted'))\n",
        "\n",
        "  print(\"Recall:\", recall_score(resultados_esperados, resultado_predicoes, average='weighted'))\n",
        "\n",
        "  print(\"Precision: \", precision_score(resultados_esperados, resultado_predicoes, average='weighted'))\n",
        "\n",
        "  tn, fp, fn, tp = confusion_matrix(resultados_esperados, resultado_predicoes).ravel()\n",
        "  print (\"True Negative: \", tn)\n",
        "  print (\"False Positive: \", fp)\n",
        "  print (\"False Negative: \", fn)\n",
        "  print (\"True Positive: \", tp)\n",
        "\n",
        "\"\"\"#Tratando os dados para chamar a validação\"\"\"\n",
        "df = pd.read_csv('/content/drive/MyDrive/Python/OFFCOMBR_CSV/OffComBR3.csv', delimiter=',')\n",
        "\n",
        "df['@@class'].replace('yes', 1, inplace=True)\n",
        "df['@@class'].replace('no', 0, inplace=True)\n",
        "\n",
        "sentencas = df['document'].values\n",
        "labels = df['@@class'].values\n",
        "\n",
        "tokenizer.enable_truncation(max_length=100)\n",
        "tokenizer.enable_padding()\n",
        "\n",
        "# TOKENINZA EM BATCH TODAS AS SENTENÇAS\n",
        "# TEM QUE USAR .TOLIST PARA CONVERTER POR LISTA. SENTENCAS É UM ARRAY NUMPY\n",
        "output = tokenizer.encode_batch(sentencas.tolist())\n",
        "\n",
        "\n",
        "# O TOKENIZER RETORAR UMA LISTA DE OBJETOS DO TIPO TOKENIZER\n",
        "# PRECISAMOS PEGAR OS ATRIBUTOS IDS E MASKS E ADICIONAR PARA LISTAS\n",
        "# OS OBJETOS TEM O ATRIBUTO IDS(IDS), TOKENS (TOKENS) E attention_mask\n",
        "# PRECISAMOS FAXER O FOR PARA PEGAR CADA UM E DEPOIS CRIAR A LISTA\n",
        "ids=[x.ids for x in output]\n",
        "tokens = [x.tokens for x in output]\n",
        "attention_mask = [x.attention_mask for x in output]\n",
        "\n",
        "\n",
        "# AGORA PRECISAMOS CONVERTER DE NUMPY PARA TENSOR\n",
        "prediction_input = torch.tensor(ids)\n",
        "prediction_mask = torch.tensor(attention_mask)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# GARANTINDO QUE TODOS OS CAMPOS TEM O MESMO TAMANHO\n",
        "print (prediction_input.size(0))\n",
        "print (prediction_mask.size(0))\n",
        "print (prediction_labels.size(0))\n",
        "\n",
        "# DEFININDO O TAMANHO DO BATCH\n",
        "batch_size = 32  \n",
        "\n",
        "# CRIANDO O DATALOADER\n",
        "prediction_data = TensorDataset(prediction_input, prediction_mask, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "Validar_Modelo(prediction_dataloader, batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "<class 'numpy.ndarray'>\n",
            "1033\n",
            "nao a pizza anunciada de anistia do caixa  sim a divulgacao da lista da odebrecht\n",
            "Tokenizer(vocabulary_size=29794, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=##)\n",
            "1033\n",
            "1033\n",
            "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "['[CLS]', 'votar', '##am', 'no', 'pe', '##za', '##o', 'agora', 'tom', '##em', 'no', 'c', '##za', '##o', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[101, 229, 22280, 122, 668, 4812, 179, 1966, 1354, 1447, 6028, 202, 3118, 171, 2504, 173, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-portuguese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-portuguese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (29794, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "Running Validation...\n",
            "  Acurácia: 0.86\n",
            "  Tempo de Validação: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "Running Validation...\n",
            "  Acurácia: 0.91\n",
            "  Tempo de Validação: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "Running Validation...\n",
            "  Acurácia: 0.92\n",
            "  Tempo de Validação: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "Running Validation...\n",
            "  Acurácia: 0.92\n",
            "  Tempo de Validação: 0:00:02\n",
            "FIM DO TREINAMENTO\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1033\n",
            "1033\n",
            "1033\n",
            "Accuracy: 0.91\n",
            "Accuracia:  0.8905852417302799\n",
            "Balanced Accuracy: 0.8104575163398693\n",
            "F1 Score: 0.8867329572015095\n",
            "Recall: 0.8905852417302799\n",
            "Precision:  0.886611891909031\n",
            "True Negative:  292\n",
            "False Positive:  14\n",
            "False Negative:  29\n",
            "True Positive:  58\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}